# ðŸ“š Further Reading â€“ Introduction to Optimization in Machine Learning

This section supports the lecture content for **Session 1: Introduction to Optimization in ML**, covering what optimization is, its key components, and the types of problems it solves.

---

## ðŸ“˜ Core Articles & Papers

- [An Overview of Gradient Descent Optimization Algorithms â€“ Sebastian Ruder (arXiv, 2016)](https://arxiv.org/abs/1609.04747)  
  A must-read overview of optimization methods, their pros/cons, and best use cases.

- [A Modern Look at Gradient Descent â€“ ArXiv (2022)](https://arxiv.org/abs/2203.11819)  
  A recent take on the theory and improvements around gradient-based optimization.

- [Understanding the Difficulty of Training Deep Feedforward Neural Networks â€“ Glorot & Bengio (2010)](https://proceedings.mlr.press/v9/glorot10a.html)  
  Highlights the importance of initialization and optimization challenges in deep nets.

---

## ðŸ§° Blogs & Visual Guides

- [A Visual Introduction to Optimization Algorithms â€“ Sebastian Raschka (2022)](https://sebastianraschka.com/blog/2022/optimizers.html)  
  Highly intuitive visual comparisons of popular optimizers.

- [Why Momentum Really Works â€“ Distill.pub](https://distill.pub/2017/momentum/)  
  A powerful visual explanation of how momentum improves convergence.

- [Google ML Crash Course: Loss and Optimization](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)  
  Practical guide with animations explaining loss minimization and gradient updates.

---

## ðŸ§ª Tutorials & Code Examples

- [Gradient Descent from Scratch â€“ Jason Brownlee](https://machinelearningmastery.com/gradient-descent-algorithm-from-scratch/)  
  Walkthrough of implementing basic gradient descent manually in Python.

- [CS231n Optimization Notes â€“ Stanford](https://cs231n.github.io/optimization-1/)  
  Excellent lecture-style notes that cover loss landscapes and gradient behaviors.

---

## ðŸ§  Bonus Insight

> "Optimization is not just a computational step â€” it's the core logic that enables models to learn from data. Choosing the right loss function and optimizer is as important as the model architecture itself."
