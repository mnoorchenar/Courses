# 🧮 Further Reading – Gradient Descent and Its Variants

This section supports **Session 2: Gradient Descent and Its Variants**, exploring standard gradient descent (GD), stochastic gradient descent (SGD), mini-batch GD, momentum, and when and why to use each variant.

---

## 📘 Core Learning Resources

These resources from Google’s ML Crash Course provide a hands-on introduction to gradient descent techniques:

- 📉 [Gradient Descent](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent)  
  Learn how gradient descent minimizes the loss function iteratively and see the impact of step size on convergence.

---

## 🧠 Bonus Insight

> *"Gradient descent is the workhorse of optimization in machine learning. Choosing the right variant for your data and model can significantly affect convergence speed and stability."*

---

## 🎨 Optional – Visualization-Rich Resources

For visual learners, these interactive and animated explanations make it easier to grasp the dynamics of various optimization methods:

- ⚡ [Distill.pub: Visualizing Gradient Descent and Momentum](https://distill.pub/2017/momentum/)  
  A vivid, interactive exploration of how gradient descent behaves with and without momentum, including friction analogies.

- 🎥 [Ruder’s Blog: An Overview of Gradient Descent Optimization Algorithms](https://www.ruder.io/optimizing-gradient-descent/)  
  A visually rich summary of GD variants including SGD, AdaGrad, RMSProp, and Adam — with intuitive diagrams and convergence paths.

- 🧪 [UCLA ACM: Gradient Descent Visualizer](https://uclaacm.github.io/gradient-descent-visualiser/#playground)  
  An interactive tool to experiment with different functions and learning rates, observing how gradient descent navigates the loss landscape.

- 📘 [Medium: A Visual Explanation of Gradient Descent Methods](https://medium.com/data-science/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)  
  An article offering visual insights into various gradient descent methods like Momentum, AdaGrad, RMSProp, and Adam.

- 📝 [SKZ Dev Blog: Gradient Descent](https://blog.skz.dev/gradient-descent)  
  A detailed walkthrough of gradient descent concepts with accompanying visualizations to enhance understanding.

---

