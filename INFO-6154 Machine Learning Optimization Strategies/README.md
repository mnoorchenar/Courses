# 📘 Machine Learning Optimization Strategies – Course Outline
| Week | Title                                                        | Key Topics Covered                                                                                                                                                  | Interactive Link                                                                                                                                                    |
|------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | 🧭 Foundations of Optimization in Machine Learning            | Optimization problem types in ML; loss surfaces, local minima, saddle points; convergence issues; real-world failures; goal framing: accuracy vs latency vs fairness | 🌀[Play with Gradient Descent](https://uclaacm.github.io/gradient-descent-visualiser/#playground)<br>📈[Linear Regression Overview](https://developers.google.com/machine-learning/crash-course/linear-regression)<br>⚖️[Loss Function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss)<br>🎯[Tune Model Parameters (Interactive)](https://developers.google.com/machine-learning/crash-course/linear-regression/parameters-exercise)|
| 2    | 🧮 Gradient Descent and Its Variants                          | Batch, stochastic, and mini-batch gradient descent; Adam, AdamW, AdaGrad, RMSProp, step decay, cosine annealing, exponential LR schedules; debugging convergence and instability; optimizer selection under noisy gradients; optimizer performance benchmarking in TensorFlow/PyTorch                   |📘[ML Optimization Overview (Google)](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)<br>🏃‍♂️[Distill – Momentum](https://distill.pub/2017/momentum/)<br>🖼️[Visual Explanation of Gradient Descent Methods (Medium)](https://medium.com/data-science/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)<br>🧠[Gradient Descent Blog (skz.dev)](https://blog.skz.dev/gradient-descent)|
| 3    | 🔍 Hyperparameter Tuning Strategies                           | Grid search, random search, validation strategies (K-fold, nested CV, hold-out); search space design (log/categorical); KerasTuner & Ray Tune intro                 | 🔁[Backpropagation Explainer](https://xnought.github.io/backprop-explainer/)<br>🧪[Playground TensorFlow](https://playground.tensorflow.org/) |
| 4    | 🧠 Bayesian Optimization & Model-Based Search                 | Bayesian optimization for black-box, expensive functions; surrogate-acquisition loop; Expected Improvement and exploration-exploitation tradeoff; TPE: modeling 𝑝(𝑥∣𝑦) via KDE; TPE vs GP: non-Gaussian, categorical-friendly; real-world tuning with Optuna (trial suggestion, pruning, visualization)                                  |📊[Optuna Dashboard (GitHub)](https://github.com/optuna/optuna-dashboard)<br>🌈[Visual Exploration of Gaussian Processes (Distill)](https://distill.pub/2019/visual-exploration-gaussian-processes/)<br>🎯[Visual Exploration of Bayesian Optimizatio (Distill)](https://distill.pub/2020/bayesian-optimization/)|
| 5    | 📊 Multi-Metric & Multi-Objective Optimization                | Accuracy vs latency, F1 vs fairness; Pareto front, scalarization techniques, budgeted optimization; real-world tradeoff modeling                                     | [Pareto Front Explorer (NSGA-II)](https://nathanrooy.github.io/posts/2020-10-01/pareto-front-visualization/)<br><br>[Multi-Objective Optimization Tool (Platypus)](https://platypus.readthedocs.io/en/latest/) |
| 6    | 🧬 Neural Network Stability & Training Dynamics               | Initialization (He, Xavier), gradient clipping, vanishing/exploding gradients, sharp vs flat minima, batch norm, layer norm, learning dynamics visualization        | [Gradient Flow Visualizer (Torch)](https://torchviz.readthedocs.io/en/latest/)<br><br>[Initialization Guide (Keras)](https://keras.io/api/layers/initializers/) |
| 7    | ⚡ Efficient & Resource-Constrained Optimization              | Tuning for mobile/edge; quantization-aware training, pruning, distillation; latency/memory profiling; TF Lite & ONNX export                                          | [TensorFlow Lite Model Optimization Toolkit](https://www.tensorflow.org/model_optimization)<br><br>[ONNX Runtime Tools](https://onnxruntime.ai/) |
| 8    | 📏 Metric Optimization and Business Alignment                 | Aligning with business KPIs (cost, churn, ROI); translating metrics into loss functions; modeling for coverage, conversion, and risk                                 | [AI Fairness Metrics Explorer (Google)](https://pair-code.github.io/what-if-tool/)<br><br>[ML Business Metrics Guide (Google Cloud)](https://cloud.google.com/blog/products/ai-machine-learning/defining-business-metrics-for-ml-models) |
| 9   | 🔍 Explainability-Driven Tuning                               | Using SHAP, LIME to guide optimization; constrained tuning (e.g., monotonicity, fairness); human-in-the-loop workflows                                               | [SHAP Visual Demo](https://shap.readthedocs.io/en/latest/example_notebooks.html)<br><br>[LIME Interactive Notebook](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20models%20with%20sklearn.html) |
| 10   | 🧰 Experiment Tracking & Optimization Visualization           | Tools: TensorBoard, Weights & Biases, MLflow; hyperparameter dashboards, loss/accuracy curves, sweep comparisons; reproducibility best practices                     | [Weights & Biases Demo](https://wandb.ai/site)<br><br>[MLflow Tracking UI](https://mlflow.org/docs/latest/tracking.html) |
| 11   | 🚀 Capstone Project: End-to-End Optimization Workflow         | Full pipeline: framing the objective, selecting optimizer, tuning, evaluating, and deploying; performance-vs-cost-vs-interpretability balancing                       | [TensorBoard Project Dashboard](https://www.tensorflow.org/tensorboard/get_started)<br><br>[ML Deployment Guide (TensorFlow)](https://www.tensorflow.org/tfx/guide/serving) |
| 13   | 🧪 Final Exam                                                 | Theory + practical case-based exam (real-world scenarios + tool usage + debugging)                                                                                    | |
| 14   | 🎤 Final Project Presentations                                | Student-led presentations: objective, tuning strategy, results analysis                                                                                               | |

## 🧾 Marking Scheme

| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

The course grading is designed to encourage consistent participation, application of learned techniques, and comprehensive project work. Each component reflects a key aspect of your learning and performance in applied machine learning optimization.
