# üìò Machine Learning Optimization Strategies ‚Äì Course Outline
| Week | Title                                                        | Key Topics Covered                                                                                                                                                   |
|------|--------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | üß≠ Foundations of Optimization in Machine Learning            | Optimization problem types in ML; loss surfaces, local minima, saddle points; convergence issues; real-world failures; goal framing: accuracy vs latency vs fairness |
| 2    | üßÆ Gradient Descent and Its Variants                          | Batch, stochastic, and mini-batch gradient descent; step decay, cosine annealing, exponential LR schedules; debugging convergence and instability                   |
| 3    | ‚öôÔ∏è Advanced Optimizers in Practice                            | Adam, AdamW, AdaGrad, RMSProp; optimizer selection under noisy gradients; optimizer performance benchmarking in TensorFlow/PyTorch                                   |
| 4    | üîç Hyperparameter Tuning Strategies                           | Grid search, random search, validation strategies (K-fold, nested CV, hold-out); search space design (log/categorical); KerasTuner & Ray Tune intro                 |
| 5    | üß† Bayesian Optimization & Model-Based Search                 | BO intuition (no math), Gaussian Processes, acquisition functions, pruning; tools like Optuna/Hyperopt; automated tuning pipelines                                  |
| 6    | üìä Multi-Metric & Multi-Objective Optimization                | Accuracy vs latency, F1 vs fairness; Pareto front, scalarization techniques, budgeted optimization; real-world tradeoff modeling                                     |
| 7    | üß¨ Neural Network Stability & Training Dynamics               | Initialization (He, Xavier), gradient clipping, vanishing/exploding gradients, sharp vs flat minima, batch norm, layer norm, learning dynamics visualization        |
| 8    | ‚ö° Efficient & Resource-Constrained Optimization              | Tuning for mobile/edge; quantization-aware training, pruning, distillation; latency/memory profiling; TF Lite & ONNX export                                          |
| 9    | üìè Metric Optimization and Business Alignment                 | Aligning with business KPIs (cost, churn, ROI); translating metrics into loss functions; modeling for coverage, conversion, and risk                                 |
| 10   | üîç Explainability-Driven Tuning                               | Using SHAP, LIME to guide optimization; constrained tuning (e.g., monotonicity, fairness); human-in-the-loop workflows                                               |
| 11   | üß∞ Experiment Tracking & Optimization Visualization           | Tools: TensorBoard, Weights & Biases, MLflow; hyperparameter dashboards, loss/accuracy curves, sweep comparisons; reproducibility best practices                     |
| 12   | üöÄ Capstone Project: End-to-End Optimization Workflow         | Full pipeline: framing the objective, selecting optimizer, tuning, evaluating, and deploying; performance-vs-cost-vs-interpretability balancing                       |
| 13   | üé§ Final Project Presentations                                | Student-led presentations: objective, tuning strategy, results analysis                                                                                               |
| 14   | üß™ Final Exam                                                 | Theory + practical case-based exam (real-world scenarios + tool usage + debugging)                                                                                    |


## üßæ Marking Scheme

| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

The course grading is designed to encourage consistent participation, application of learned techniques, and comprehensive project work. Each component reflects a key aspect of your learning and performance in applied machine learning optimization.
