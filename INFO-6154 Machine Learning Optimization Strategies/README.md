# ğŸ“˜ Machine Learning Optimization Strategies â€“ Course Outline
| Week | Title                                                        | Key Topics Covered                                                                                                                                                  | Interactive Link                                                                                                                                                    |
|------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | ğŸ§­ Foundations of Optimization in Machine Learning            | Optimization problem types in ML; loss surfaces, local minima, saddle points; convergence issues; real-world failures; goal framing: accuracy vs latency vs fairness | ğŸŒ€[Play with Gradient Descent](https://uclaacm.github.io/gradient-descent-visualiser/#playground)<br>ğŸ“ˆ[Linear Regression Overview](https://developers.google.com/machine-learning/crash-course/linear-regression)<br>âš–ï¸[Loss Function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss)<br>ğŸ¯[Tune Model Parameters (Interactive)](https://developers.google.com/machine-learning/crash-course/linear-regression/parameters-exercise)|
| 2    | ğŸ§® Gradient Descent and Its Variants                          | Batch, stochastic, and mini-batch gradient descent; Adam, AdamW, AdaGrad, RMSProp, step decay, cosine annealing, exponential LR schedules; debugging convergence and instability; optimizer selection under noisy gradients; optimizer performance benchmarking in TensorFlow/PyTorch                   |ğŸ“˜[ML Optimization Overview (Google)](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)<br>ğŸƒâ€â™‚ï¸[Distill â€“ Momentum](https://distill.pub/2017/momentum/)<br>ğŸ–¼ï¸[Visual Explanation of Gradient Descent Methods (Medium)](https://medium.com/data-science/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)<br>ğŸ§ [Gradient Descent Blog (skz.dev)](https://blog.skz.dev/gradient-descent)|
| 3    | ğŸ” Hyperparameter Tuning Strategies                           | Grid search, random search, validation strategies (K-fold, nested CV, hold-out); search space design (log/categorical); KerasTuner & Ray Tune intro                 | ğŸ”[Backpropagation Explainer](https://xnought.github.io/backprop-explainer/)<br>ğŸ§ª[Playground TensorFlow](https://playground.tensorflow.org/) |
| 4    | ğŸ§  Bayesian Optimization & Model-Based Search                 | Bayesian optimization for black-box, expensive functions; surrogate-acquisition loop; Expected Improvement and exploration-exploitation tradeoff; TPE: modeling ğ‘(ğ‘¥âˆ£ğ‘¦) via KDE; TPE vs GP: non-Gaussian, categorical-friendly; real-world tuning with Optuna (trial suggestion, pruning, visualization)                                  |ğŸ“Š[Optuna Dashboard (GitHub)](https://github.com/optuna/optuna-dashboard)<br>ğŸŒˆ[Visual Exploration of Gaussian Processes (Distill)](https://distill.pub/2019/visual-exploration-gaussian-processes/)<br>ğŸ¯[Visual Exploration of Bayesian Optimizatio (Distill)](https://distill.pub/2020/bayesian-optimization/)|
| 5    | ğŸ“Š Multi-Metric & Multi-Objective Optimization                | Accuracy vs latency, F1 vs fairness; Pareto front, scalarization techniques, budgeted optimization; real-world tradeoff modeling                                     | [Pareto Front Explorer (NSGA-II)](https://nathanrooy.github.io/posts/2020-10-01/pareto-front-visualization/)<br><br>[Multi-Objective Optimization Tool (Platypus)](https://platypus.readthedocs.io/en/latest/) |
| 6    | ğŸ§¬ Neural Network Stability & Training Dynamics               | Initialization (He, Xavier), gradient clipping, vanishing/exploding gradients, sharp vs flat minima, batch norm, layer norm, learning dynamics visualization        | [Gradient Flow Visualizer (Torch)](https://torchviz.readthedocs.io/en/latest/)<br><br>[Initialization Guide (Keras)](https://keras.io/api/layers/initializers/) |
| 7    | âš¡ Efficient & Resource-Constrained Optimization              | Tuning for mobile/edge; quantization-aware training, pruning, distillation; latency/memory profiling; TF Lite & ONNX export                                          | [TensorFlow Lite Model Optimization Toolkit](https://www.tensorflow.org/model_optimization)<br><br>[ONNX Runtime Tools](https://onnxruntime.ai/) |
| 8    | ğŸ“ Metric Optimization and Business Alignment                 | Aligning with business KPIs (cost, churn, ROI); translating metrics into loss functions; modeling for coverage, conversion, and risk                                 | [AI Fairness Metrics Explorer (Google)](https://pair-code.github.io/what-if-tool/)<br><br>[ML Business Metrics Guide (Google Cloud)](https://cloud.google.com/blog/products/ai-machine-learning/defining-business-metrics-for-ml-models) |
| 9   | ğŸ” Explainability-Driven Tuning                               | Using SHAP, LIME to guide optimization; constrained tuning (e.g., monotonicity, fairness); human-in-the-loop workflows                                               | [SHAP Visual Demo](https://shap.readthedocs.io/en/latest/example_notebooks.html)<br><br>[LIME Interactive Notebook](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20models%20with%20sklearn.html) |
| 10   | ğŸ§° Experiment Tracking & Optimization Visualization           | Tools: TensorBoard, Weights & Biases, MLflow; hyperparameter dashboards, loss/accuracy curves, sweep comparisons; reproducibility best practices                     | [Weights & Biases Demo](https://wandb.ai/site)<br><br>[MLflow Tracking UI](https://mlflow.org/docs/latest/tracking.html) |
| 11   | ğŸš€ Capstone Project: End-to-End Optimization Workflow         | Full pipeline: framing the objective, selecting optimizer, tuning, evaluating, and deploying; performance-vs-cost-vs-interpretability balancing                       | [TensorBoard Project Dashboard](https://www.tensorflow.org/tensorboard/get_started)<br><br>[ML Deployment Guide (TensorFlow)](https://www.tensorflow.org/tfx/guide/serving) |
| 13   | ğŸ§ª Final Exam                                                 | Theory + practical case-based exam (real-world scenarios + tool usage + debugging)                                                                                    | |
| 14   | ğŸ¤ Final Project Presentations                                | Student-led presentations: objective, tuning strategy, results analysis                                                                                               | |

## ğŸ§¾ Marking Scheme

| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

The course grading is designed to encourage consistent participation, application of learned techniques, and comprehensive project work. Each component reflects a key aspect of your learning and performance in applied machine learning optimization.
