# üìò Machine Learning Optimization Strategies ‚Äì Course Outline
| Week | Title                                                        | Key Topics Covered                                                                                                                                                  | Interactive Link                                                                                                                                                    |
|------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | üß≠ Foundations of Optimization in Machine Learning            | Optimization problem types in ML; loss surfaces, local minima, saddle points; convergence issues; real-world failures; goal framing: accuracy vs latency vs fairness | üåÄ[Play with Gradient Descent](https://uclaacm.github.io/gradient-descent-visualiser/#playground)<br>üß™[Playground TensorFlow](https://playground.tensorflow.org/)<br>üìà[Linear Regression Overview](https://developers.google.com/machine-learning/crash-course/linear-regression)<br>‚öñÔ∏è[Loss Function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss)<br>üéØ[Tune Model Parameters (Interactive)](https://developers.google.com/machine-learning/crash-course/linear-regression/parameters-exercise)|
| 2    | üßÆ Gradient Descent and Its Variants                          | Batch, stochastic, and mini-batch gradient descent; Adam, AdamW, AdaGrad, RMSProp, step decay, cosine annealing, exponential LR schedules; debugging convergence and instability; optimizer selection under noisy gradients; optimizer performance benchmarking in TensorFlow/PyTorch                   | [ML Optimization Overview (Google)](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)<br><br>[Distill - Momentum](https://distill.pub/2017/momentum/) |
| 3    | üîç Hyperparameter Tuning Strategies                           | Grid search, random search, validation strategies (K-fold, nested CV, hold-out); search space design (log/categorical); KerasTuner & Ray Tune intro                 | [KerasTuner Demo](https://keras.io/keras_tuner/)<br><br>[Ray Tune Docs](https://docs.ray.io/en/latest/tune/index.html) |
| 4    | üß† Bayesian Optimization & Model-Based Search                 | BO intuition (no math), Gaussian Processes, acquisition functions, pruning; tools like Optuna/Hyperopt; automated tuning pipelines                                  | [Optuna Visualization Dashboard](https://optuna.org/#demo)<br><br>[Hyperopt Introduction (GitHub)](https://github.com/hyperopt/hyperopt) |
| 5    | üìä Multi-Metric & Multi-Objective Optimization                | Accuracy vs latency, F1 vs fairness; Pareto front, scalarization techniques, budgeted optimization; real-world tradeoff modeling                                     | [Pareto Front Explorer (NSGA-II)](https://nathanrooy.github.io/posts/2020-10-01/pareto-front-visualization/)<br><br>[Multi-Objective Optimization Tool (Platypus)](https://platypus.readthedocs.io/en/latest/) |
| 6    | üß¨ Neural Network Stability & Training Dynamics               | Initialization (He, Xavier), gradient clipping, vanishing/exploding gradients, sharp vs flat minima, batch norm, layer norm, learning dynamics visualization        | [Gradient Flow Visualizer (Torch)](https://torchviz.readthedocs.io/en/latest/)<br><br>[Initialization Guide (Keras)](https://keras.io/api/layers/initializers/) |
| 7    | ‚ö° Efficient & Resource-Constrained Optimization              | Tuning for mobile/edge; quantization-aware training, pruning, distillation; latency/memory profiling; TF Lite & ONNX export                                          | [TensorFlow Lite Model Optimization Toolkit](https://www.tensorflow.org/model_optimization)<br><br>[ONNX Runtime Tools](https://onnxruntime.ai/) |
| 8    | üìè Metric Optimization and Business Alignment                 | Aligning with business KPIs (cost, churn, ROI); translating metrics into loss functions; modeling for coverage, conversion, and risk                                 | [AI Fairness Metrics Explorer (Google)](https://pair-code.github.io/what-if-tool/)<br><br>[ML Business Metrics Guide (Google Cloud)](https://cloud.google.com/blog/products/ai-machine-learning/defining-business-metrics-for-ml-models) |
| 9   | üîç Explainability-Driven Tuning                               | Using SHAP, LIME to guide optimization; constrained tuning (e.g., monotonicity, fairness); human-in-the-loop workflows                                               | [SHAP Visual Demo](https://shap.readthedocs.io/en/latest/example_notebooks.html)<br><br>[LIME Interactive Notebook](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20models%20with%20sklearn.html) |
| 10   | üß∞ Experiment Tracking & Optimization Visualization           | Tools: TensorBoard, Weights & Biases, MLflow; hyperparameter dashboards, loss/accuracy curves, sweep comparisons; reproducibility best practices                     | [Weights & Biases Demo](https://wandb.ai/site)<br><br>[MLflow Tracking UI](https://mlflow.org/docs/latest/tracking.html) |
| 11   | üöÄ Capstone Project: End-to-End Optimization Workflow         | Full pipeline: framing the objective, selecting optimizer, tuning, evaluating, and deploying; performance-vs-cost-vs-interpretability balancing                       | [TensorBoard Project Dashboard](https://www.tensorflow.org/tensorboard/get_started)<br><br>[ML Deployment Guide (TensorFlow)](https://www.tensorflow.org/tfx/guide/serving) |
| 13   | üß™ Final Exam                                                 | Theory + practical case-based exam (real-world scenarios + tool usage + debugging)                                                                                    | |
| 14   | üé§ Final Project Presentations                                | Student-led presentations: objective, tuning strategy, results analysis                                                                                               | |

## üßæ Marking Scheme

| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

The course grading is designed to encourage consistent participation, application of learned techniques, and comprehensive project work. Each component reflects a key aspect of your learning and performance in applied machine learning optimization.
