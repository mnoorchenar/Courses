# üìò Deep Learning with Tensorflow & Kersa 2 ‚Äì Course Outline

| Week | Title                                                                 | Key Topics Covered| Interactive Link |
|------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| 1    | üß† Introduction to ANNs and Limitations in Image Processing          | Review of Artificial Neural Networks (ANNs): components (neurons, layers, weights/bias, activation functions, loss functions, optimizers); practical ANN implementation in Keras (model creation, compilation, training, and evaluation); digit recognition case study with MNIST dataset; performance metrics (accuracy, precision, recall); limitations of ANNs with high-dimensional image data (lack of spatial awareness, computational cost, overfitting, translation invariance); need for CNNs in image processing |[ConvNetJS MNIST Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)<br><br>[Neural Network Playground (TensorFlow)](https://playground.tensorflow.org/)|
| 2    | üß± Fundamentals of CNNs and Hierarchical Feature Learning            | Introduction to CNNs and their advantages in image processing; structural design and grid data processing; core CNN components (convolutional, pooling, activation, fully connected, regularization); hierarchical feature learning (low-, mid-, high-level features); key CNN capabilities (translation invariance, parameter sharing, sparsity); comparison with ANNs for large image data; working pipeline of a CNN (preprocessing, layers, optimization); application domains (medical imaging, GIS, signal processing, video, seismic data); real-world models: LeNet, AlexNet, VGG, ResNet, Inception |[CNN Explainer (TensorFlow)](https://poloclub.github.io/cnn-explainer/)<br><br>[CS231n Convolutional Visualizations](https://cs231n.github.io/understanding-cnn/)|
| 3    | ‚öôÔ∏è CNN Architecture, OpenCV & Data Augmentation                     | Convolutional layers and kernel operations; stride and padding types ('valid' vs. 'same'); pooling layers (max, average, global); forward and backpropagation processes; regularization techniques (dropout, batch normalization); OpenCV for image loading, preprocessing (resizing, normalization, color space conversion), and data augmentation; feature extraction (Canny, Sobel, Harris corners); visualizing images, filters, annotations; CNN implementation with TensorFlow on MNIST and CIFAR-10 datasets; TensorFlow + OpenCV integration pipeline; image augmentation using ImageDataGenerator |[OpenCV Interactive Tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)<br><br>[Albumentations Image Augmentation Playground](https://albumentations.ai/)|
| 4    | ‚öôÔ∏è CNN Applications, Transfer Learning & Object Detection           | Real-world applications of CNNs: image classification, object detection, segmentation; CNN model pipeline using TensorFlow and Keras; practical demo of image classification (fruit dataset); architecture components (input, convolutional, pooling, flatten, dense, output layers); training workflow (data normalization, augmentation, forward/backpropagation, loss, optimizer); evaluation metrics (accuracy, precision, recall, F1, confusion matrix); advanced CNN architectures: VGG16/19, ResNet, Inception, MobileNet; trade-offs in complexity, performance, and use cases; intro to transfer learning with pretrained models (ResNet50); object detection: R-CNN, Fast R-CNN, Faster R-CNN, YOLO; bounding box regression, IoU, confidence scoring, Non-Max Suppression (NMS); performance optimization |[YOLO Object Detection Demo](https://pjreddie.com/darknet/yolo/)<br><br>[TensorFlow Hub Image Classifier](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5)|
| 5    | üîÅ Recurrent Neural Networks ‚Äì Concepts & Applications              | Limitations of CNNs for sequence modeling; introduction to RNNs and their structure; information flow through unrolled RNNs; forward and backpropagation in RNNs; issues in training (vanishing/exploding gradients); mitigation strategies (gradient clipping, truncated BPTT, RMSprop); hidden states, weight sharing, and temporal dependencies; activation functions (tanh, ReLU); SimpleRNN implementation in Keras; time series forecasting with synthetic stock data; real-world RNN applications: time series forecasting (finance, weather), music generation, medical diagnostics, video frame prediction; combining CNNs with RNNs (CNN‚ÜíRNN, RNN-CNN); case study on glaucoma detection using fundus image sequences; CNN-only vs. CNN+RNN performance (F-measure); spatial vs. temporal feature trade-offs |[RNN Visual Explanation](https://grus.io/recurrence/)<br><br>[Understanding RNNs (Colah‚Äôs Blog)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)|
| 6    | üîÑ LSTM Networks ‚Äì Deep Dive & Practical Modeling                   | Challenges in RNN training (vanishing/exploding gradients); BPTT and weight sharing; introduction to LSTM architecture: cell state, hidden state, forget/input/output gates; gate mechanics and equations; how LSTMs mitigate gradient issues; step-by-step cell computation and data flow; LSTM types: vanilla, stacked, bidirectional; use cases in long-sequence modeling; hands-on Keras implementation and training; performance comparison with basic RNNs |[LSTM Cell Visualizer](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)<br><br>[LSTM Playground (Keras)](https://keras.io/examples/timeseries/timeseries_weather_forecasting/)|
| 7    | üîÑ GRU Networks & Comparative RNN Modeling                         | GRU architecture: reset and update gates; detailed flow of GRU forward pass; benefits of GRU over LSTM in efficiency and performance; practical implementation of GRUs in Keras; sequence prediction case study (integer series); CNN-LSTM hybrid modeling; sine-cosine sequence modeling with RNN, LSTM, and GRU; training loss, validation, memory usage; conclusions on architecture trade-offs |[GRU vs LSTM Comparison](https://towardsdatascience.com/lstm-vs-gru-experimental-comparison-955820c21e8b)<br><br>[Sequence Modeling with RNN/GRU (TensorFlow)](https://www.tensorflow.org/tutorials/text/text_generation)|
| 8    | ‚öñÔ∏è LSTM vs GRU: Weight Sharing & Efficiency in RNNs                | Weight-sharing principles in RNN, LSTM, and GRU (time step consistency, gate-specific weights); architectural comparison of LSTM (input, forget, output gates) vs GRU (update, reset gates); demonstration of weight-sharing and hidden state behavior; limitations of LSTMs (complexity, overfitting, training difficulty); GRU advantages (fewer parameters, regularization, speed, hardware efficiency); real-world training implications |[Weight Sharing in RNNs (Distill)](https://distill.pub/2019/memorization-in-rnns/)<br><br>[Advanced RNN Visualization (Harvard NLP)](http://nlp.seas.harvard.edu/2018/04/03/attention.html)|
| 9    | üëÅÔ∏è Attention Mechanisms and Transformer Foundations               | Limitations of RNNs and LSTMs in modeling long-range dependencies; introduction to attention mechanisms (selective focus, human cognition analogy); types of attention: self-attention, multi-head, additive (Bahdanau), and scaled dot-product (Luong); attention score functions (dot product, additive, multiplicative); context vectors and their computation; encoder-decoder architecture; role of weighted sums in capturing sequence context; applications in NLP, time series, and image captioning |[Attention Visualization (Jalammar)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)<br><br>[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)|
| 10   | ü§ñ Transformer Models and Vision Transformers (ViT)               | Transformer architecture and its components (self-attention, multi-head attention, feedforward layers, residual connections, layer normalization); encoder-decoder structure; positional encoding and its role in sequence modeling; comparison to RNNs and CNNs; vision-specific adaptation via Vision Transformers (ViT); implementation of Transformers in TensorFlow/Keras using MultiHeadAttention; CIFAR-10 classification with custom and pretrained ViT models; fine-tuning and transfer learning |[ViT Playground (Hugging Face)](https://huggingface.co/blog/vit)<br><br>[Vision Transformer Demo (Google Research)](https://vit-demo.com/)|
| 11   | üß™ Generative Adversarial Networks (GANs)                         | Introduction to generative models vs. discriminative models; GAN structure: generator and discriminator roles; adversarial training process and minimax game; challenges (mode collapse, vanishing gradients); key GAN variants: cGANs, DCGANs, StyleGAN, CycleGAN, InfoGAN; architecture and training algorithm walkthrough; applications in image generation, style transfer, data augmentation, and synthetic medical data; Alzheimer's case study using GANs with multi-modal brain imaging and multi-loop learning |[GAN Lab (TensorFlow)](https://poloclub.github.io/ganlab/)<br><br>[DCGAN Hands-on Notebook (TensorFlow)](https://www.tensorflow.org/tutorials/generative/dcgan)|
| 12   | üß™ GAN Implementation & Case Study (Alzheimer‚Äôs Diagnosis)       | GAN training algorithm walkthrough: discriminator and generator update cycles; dataset generation using random noise; detailed loss function tuning; backpropagation of adversarial loss; case study using BNLoop-GAN for Alzheimer's classification from MRI data; multi-loop learning algorithm; .accuracy, sensitivity, specificity metrics; performance comparisons with single-modal vs. multi-modal networks; discussion of generalization in healthcare GAN models |[GANs in Medical Imaging (DeepImaging)](https://deepimaging.io/projects/)<br><br>[Alzheimer's Diagnosis with AI (MIT News)](https://news.mit.edu/2019/deep-learning-alzheimers-disease-diagnosis-0619)|
| 13   | üé§ Final Project Presentations                                    | Student presentations on final projects ||
| 14   | üß™ Final Exam                                                     | theory and practical ||

## üßæ Marking Scheme
| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

---

This course explores advanced deep learning concepts through hands-on implementation using TensorFlow and Keras. Students will gain experience with CNNs, RNNs (including LSTM and GRU), Transformer models, and GANs, while solving real-world problems and presenting their solutions.
