# üìò Deep Learning with Tensorflow & Kersa 2 ‚Äì Course Outline

| Week | Title                                                                 | Key Topics Covered| Interactive Link |
|------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| 1    | üß† Introduction to ANNs and Limitations in Image Processing          | Review of Artificial Neural Networks (ANNs): components (neurons, layers, weights/bias, activation functions, loss functions, optimizers); practical ANN implementation in Keras (model creation, compilation, training, and evaluation); digit recognition case study with MNIST dataset; performance metrics (accuracy, precision, recall); limitations of ANNs with high-dimensional image data (lack of spatial awareness, computational cost, overfitting, translation invariance); need for CNNs in image processing |[ConvNetJS MNIST Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)<br><br>[Neural Network Playground (TensorFlow)](https://playground.tensorflow.org/)|
| 2   | üß± Fundamentals of CNNs and Hierarchical Feature Learning            | Introduction to CNNs and their advantages in image processing; structural design and grid data processing; core CNN components (convolutional, pooling, activation, fully connected, regularization); hierarchical feature learning (low-, mid-, high-level features); key CNN capabilities (translation invariance, parameter sharing, sparsity); comparison with ANNs for large image data; working pipeline of a CNN (preprocessing, layers, optimization); application domains (medical imaging, GIS, signal processing, video, seismic data); real-world models: LeNet, AlexNet, VGG, ResNet, Inception |[CNN Explainer (TensorFlow)](https://poloclub.github.io/cnn-explainer/)<br><br>[Github Convolutional Visualizations](https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network)|
| 3    | ‚öôÔ∏è CNN Architecture, OpenCV & Data Augmentation                     | Convolutional layers and kernel operations; stride and padding types ('valid' vs. 'same'); pooling layers (max, average, global); forward and backpropagation processes; regularization techniques (dropout, batch normalization); OpenCV for image loading, preprocessing (resizing, normalization, color space conversion), and data augmentation; feature extraction (Canny, Sobel, Harris corners); visualizing images, filters, annotations; CNN implementation with TensorFlow on MNIST and CIFAR-10 datasets; TensorFlow + OpenCV integration pipeline; image augmentation using ImageDataGenerator |[OpenCV Interactive Tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)<br><br>[Albumentations Image Augmentation Playground](https://albumentations.ai/)|
| 4-5    | ‚öôÔ∏è CNN Applications, Transfer Learning & Object Detection           | Real-world applications of CNNs: image classification, object detection, segmentation; CNN model pipeline using TensorFlow and Keras; practical demo of image classification (fruit dataset); architecture components (input, convolutional, pooling, flatten, dense, output layers); training workflow (data normalization, augmentation, forward/backpropagation, loss, optimizer); evaluation metrics (accuracy, precision, recall, F1, confusion matrix); advanced CNN architectures: VGG16/19, ResNet, Inception, MobileNet; trade-offs in complexity, performance, and use cases; intro to transfer learning with pretrained models (ResNet50); object detection: R-CNN, Fast R-CNN, Faster R-CNN, YOLO; bounding box regression, IoU, confidence scoring, Non-Max Suppression (NMS); performance optimization |[YOLO Object Detection Demo](https://pjreddie.com/darknet/yolo/)<br><br>[TensorFlow Hub Image Classifier](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5)|
| 6    | üîÅ Recurrent Neural Networks ‚Äì Concepts & Applications              | Limitations of CNNs for sequence modeling; introduction to RNNs and their structure; information flow through unrolled RNNs; forward and backpropagation in RNNs; issues in training (vanishing/exploding gradients); mitigation strategies (gradient clipping, truncated BPTT, RMSprop); hidden states, weight sharing, and temporal dependencies; activation functions (tanh, ReLU); SimpleRNN implementation in Keras; time series forecasting with synthetic stock data; real-world RNN applications: time series forecasting (finance, weather), music generation, medical diagnostics, video frame prediction; combining CNNs with RNNs (CNN‚ÜíRNN, RNN-CNN); case study on glaucoma detection using fundus image sequences; CNN-only vs. CNN+RNN performance (F-measure); spatial vs. temporal feature trade-offs |[Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)<br><br>[Understanding RNNs (Colah‚Äôs Blog)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)|
| 7    | üîÑ LSTM & GRU ‚Äì Gated Architectures and Comparative Modeling                   | LSTM motivation and limitations of RNNs;Gating mechanisms in recurrent networks; LSTM architecture: cell state, hidden state, forget/input/output gates; Python implementation: LSTM cell structure walkthrough using custom code or Keras functional API; Data flow and gate interaction in LSTM cells; LSTM variants: vanilla, stacked, bidirectional; Python implementation: Vanilla vs. stacked vs. bidirectional LSTM with Keras (on small sequence dataset); GRU architecture: reset and update gates; Python implementation: GRU structure and gate behavior in Keras; Differences between LSTM and GRU (parameters, gating, structure); GRU variants: vanilla and stacked; Python implementation: stacked GRU example and comparison with LSTM model; Forward pass and information flow in GRU cells; Advantages and trade-offs: LSTM vs GRU in accuracy, training time, and efficiency; Python implementation: model comparison (LSTM vs GRU) on same dataset ‚Äì training time, loss, memory; Model selection strategy: when to use RNN, LSTM, or GRU; CNN-LSTM hybrid mention and connection to prior CNN+RNN topics; Python implementation: brief example of CNN‚ÜíLSTM pipeline (e.g., image sequence classification); Final comparison and summary: RNN vs LSTM vs GRU |[LSTM Cell Visualizer](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)<br><br>[LSTM Playground (Keras)](https://keras.io/examples/timeseries/timeseries_weather_forecasting/)<br><br>[Sequence Modeling with RNN/GRU (TensorFlow)](https://www.tensorflow.org/tutorials/text/text_generation)|
| 8-9    | üëÅÔ∏è Attention Mechanisms and Transformer Foundations               | Limitations of RNNs and LSTMs in modeling long-range dependencies; introduction to attention mechanisms (selective focus, human cognition analogy); types of attention: self-attention, multi-head, additive (Bahdanau), and scaled dot-product (Luong); attention score functions (dot product, additive, multiplicative); context vectors and their computation; encoder-decoder architecture; role of weighted sums in capturing sequence context; applications in NLP, time series, and image captioning |[Attention Visualization (Jalammar)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)<br><br>[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)|
| 10   | ü§ñ Transformer Models and Vision Transformers (ViT)               | Transformer architecture and its components (self-attention, multi-head attention, feedforward layers, residual connections, layer normalization); encoder-decoder structure; positional encoding and its role in sequence modeling; comparison to RNNs and CNNs; vision-specific adaptation via Vision Transformers (ViT); implementation of Transformers in TensorFlow/Keras using MultiHeadAttention; CIFAR-10 classification with custom and pretrained ViT models; fine-tuning and transfer learning |[ViT Playground (Hugging Face)](https://huggingface.co/blog/vit)<br><br>[Vision Transformer Demo (Google Research)](https://vit-demo.com/)|
| 11   | üß™ Generative Adversarial Networks (GANs)                         | Introduction to generative models vs. discriminative models; GAN structure: generator and discriminator roles; adversarial training process and minimax game; challenges (mode collapse, vanishing gradients); key GAN variants: cGANs, DCGANs, StyleGAN, CycleGAN, InfoGAN; architecture and training algorithm walkthrough; applications in image generation, style transfer, data augmentation, and synthetic medical data; Alzheimer's case study using GANs with multi-modal brain imaging and multi-loop learning |[GAN Lab (TensorFlow)](https://poloclub.github.io/ganlab/)<br><br>[DCGAN Hands-on Notebook (TensorFlow)](https://www.tensorflow.org/tutorials/generative/dcgan)|
| 12   | üß™ GAN Implementation & Case Study (Alzheimer‚Äôs Diagnosis)       | GAN training algorithm walkthrough: discriminator and generator update cycles; dataset generation using random noise; detailed loss function tuning; backpropagation of adversarial loss; case study using BNLoop-GAN for Alzheimer's classification from MRI data; multi-loop learning algorithm; .accuracy, sensitivity, specificity metrics; performance comparisons with single-modal vs. multi-modal networks; discussion of generalization in healthcare GAN models |[GANs in Medical Imaging (DeepImaging)](https://deepimaging.io/projects/)<br><br>[Alzheimer's Diagnosis with AI (MIT News)](https://news.mit.edu/2019/deep-learning-alzheimers-disease-diagnosis-0619)|
| 13   | üé§ Final Project Presentations                                    | Student presentations on final projects ||
| 14   | üß™ Final Exam                                                     | theory and practical ||

## üßæ Marking Scheme
| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

---

This course explores advanced deep learning concepts through hands-on implementation using TensorFlow and Keras. Students will gain experience with CNNs, RNNs (including LSTM and GRU), Transformer models, and GANs, while solving real-world problems and presenting their solutions.
