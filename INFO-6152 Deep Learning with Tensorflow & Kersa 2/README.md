# ğŸ“˜ Deep Learning with TensorFlow and Keras 2 â€“ Course Outline

| Week | Title                                                                 | Key Topics Covered                                                                                                                                                      |
|------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | ğŸ§  Intro to ANNs and Image Data Limitations                          | Overview of Artificial Neural Networks (ANNs) and their limitations in handling large image datasets                                                                    |
| 2    | ğŸ§± Fundamentals of CNNs                                               | CNN architecture, key components, and their applications in image processing                                                                                            |
| 3    | âš™ï¸ Working Principles of CNNs (Part 1)                                | Convolutional layers, operations, stride, padding, pooling, fully connected layers, forward/backpropagation, training, regularization techniques                         |
| 4    | âš™ï¸ Working Principles of CNNs (Part 2)                                | Real-world applications using TensorFlow/Keras, CNN models like ResNet, Inception, and step-by-step implementation                                                      |
| 5    | ğŸ” Introduction to RNNs and Sequential Data                          | Feedforward NN limitations, RNN structure, unrolling, activation functions                                                                                              |
| 6    | ğŸ” RNNs: Concepts & Implementation                                   | Capturing sequential dependencies, practical examples, strengths/weaknesses                                                                                             |
| 7    | â¸ï¸ Reading Week                                                      | No class                                                                                                                                                                |
| 8    | ğŸ¼ Applications of RNNs                                              | Time series forecasting (finance, weather), music generation, video frame prediction, healthcare applications                                                           |
| 9    | ğŸ”„ Advanced RNNs: LSTM & GRU (Part 1)                                | RNN limitations, LSTM intro and architecture, long-range dependency learning                                                                                           |
| 10   | ğŸ”„ Advanced RNNs: LSTM & GRU (Part 2)                                | GRU intro, comparison with LSTM, BPTT training, optimization, real-world applications (text, speech, time series)                                                       |
| 11   | ğŸ‘ï¸ Attention and Transformers (Part 1)                              | Attention mechanisms, types of attention, score functions, context vectors, Transformer intro, encoder-decoder, positional encoding                                     |
| 12   | ğŸ‘ï¸ Attention and Transformers (Part 2)                              | Transformer deep dive: self-attention, multi-head attention, training, feed-forward networks, normalization, applications                                               |
| 13   | ğŸ§ª Generative Adversarial Networks (GANs)                           | GAN intro, applications, architecture, training, loss design, variants: Conditional GANs, DCGANs, StyleGAN, etc.                                                        |
| 14   | ğŸ› ï¸ Building & Evaluating GANs                                       | GANs for synthetic data, environment setup, architecture design, model implementation in TensorFlow/Keras                                                              |
| 15   | ğŸ¤ Final Project Presentations                                       | Student presentations on final projects                                                                                                                                |

## ğŸ§¾ Marking Scheme

| Component           | Weight  |
|--------------------|---------|
| In-Class Activities (4 total) | 20%     |
| Quizzes (3 required + 1 optional) | 30%     |
| Final Project                 | 30%     |
| Final Presentation + Q&A     | 20%     |

### ğŸ“š Quizzes
- **Quiz 1 (10%)** â€“ Based on Weeks 1â€“3  
- **Quiz 2 (10%)** â€“ Based on Weeks 4â€“7  
- **Quiz 3 (10%)** â€“ Based on Weeks 9â€“12  
- **Optional Quiz 4 (10%)** â€“ Based on Weeks 13â€“14  

### ğŸ§‘â€ğŸ« In-Class Activities
- **Activity 1 (5%)** â€“ Weeks 1â€“2  
- **Activity 2 (5%)** â€“ Weeks 4â€“6  
- **Activity 3 (5%)** â€“ Weeks 8â€“10  
- **Activity 4 (5%)** â€“ Weeks 11â€“12  

---

## ğŸš€ Final Project â€“ 30%

Students will design, implement, and evaluate an end-to-end deep learning solution using **TensorFlow and Keras**, selecting one or more of the following architectures: **CNN, LSTM, GRU, Transformer, or GAN**.

Final deliverables must include:

- A clear, well-structured **written report** explaining:
  - Problem statement and objective
  - Data processing and model design decisions
  - Coding logic and implementation flow
  - Observations and lessons learned

- **Well-documented source code** answering all relevant coding tasks and questions provided

- A **Results Section** clearly detailing:
  - Evaluation metrics and outcomes
  - Visualizations (if applicable)
  - Interpretations of performance

- A concise **oral presentation and Q&A session** during the final week (Week 15)

---

This course explores advanced deep learning concepts through hands-on implementation using TensorFlow and Keras. Students will gain experience with CNNs, RNNs (including LSTM and GRU), Transformer models, and GANs, while solving real-world problems and presenting their solutions.