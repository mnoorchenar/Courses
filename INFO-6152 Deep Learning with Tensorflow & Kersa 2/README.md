# üìò Deep Learning with TensorFlow and Keras 2 ‚Äì Course Outline

| Week | Title                                                                 | Key Topics Covered                                                                                                                                                      |
|------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | üß† Introduction to ANNs and Limitations in Image Processing         | Review of Artificial Neural Networks (ANNs): components (neurons, layers, weights/bias, activation functions, loss functions, optimizers); practical ANN implementation in Keras (model creation, compilation, training, and evaluation); digit recognition case study with MNIST dataset; performance metrics (accuracy, precision, recall); limitations of ANNs with high-dimensional image data (lack of spatial awareness, computational cost, overfitting, translation invariance); need for CNNs in image processing |
| 2    | üß± Fundamentals of CNNs and Hierarchical Feature Learning           | Introduction to CNNs and their advantages in image processing; structural design and grid data processing; core CNN components (convolutional, pooling, activation, fully connected, regularization); hierarchical feature learning (low-, mid-, high-level features); key CNN capabilities (translation invariance, parameter sharing, sparsity); comparison with ANNs for large image data; working pipeline of a CNN (preprocessing, layers, optimization); application domains (medical imaging, GIS, signal processing, video, seismic data); real-world models: LeNet, AlexNet, VGG, ResNet, Inception |
| 3    | ‚öôÔ∏è Working Principles of CNNs ‚Äì Core Architecture & OpenCV Integration | Convolutional layers and kernel operations; stride and padding types ('valid' vs. 'same'); pooling layers (max, average, global); forward and backpropagation processes; regularization techniques (dropout, batch normalization); OpenCV for image loading, preprocessing (resizing, normalization, color space conversion), and data augmentation; feature extraction (Canny, Sobel, Harris corners); visualizing images, filters, annotations; CNN implementation with TensorFlow on MNIST and CIFAR-10 datasets; TensorFlow + OpenCV integration pipeline; image augmentation using ImageDataGenerator |
| 4    | ‚öôÔ∏è CNN Applications and Basic Architecture                     | Real-world applications of CNNs: image classification, object detection, segmentation; CNN model pipeline using TensorFlow and Keras; practical demo of image classification (fruit dataset); architecture components (input, convolutional, pooling, flatten, dense, output layers); training workflow (data normalization, augmentation, forward/backpropagation, loss, optimizer); evaluation metrics (accuracy, precision, recall, F1, confusion matrix); basic OpenCV image preprocessing |
| 5    | üì¶ Advanced CNN Models & Transfer Learning                    | Advanced CNN architectures: VGG16/19, ResNet, Inception, MobileNet; trade-offs in complexity, performance, and use cases; introduction to transfer learning: fine-tuning pretrained models (e.g., ResNet50); demo on model reuse and feature extraction; object detection techniques: R-CNN, Fast R-CNN, Faster R-CNN, YOLO; bounding box regression, Intersection-over-Union (IoU), confidence scoring, Non-Max Suppression (NMS); performance optimization for deployment |
| 6    | üîÅ Recurrent Neural Networks: Concepts and Implementation      | Limitations of CNNs for sequence modeling; introduction to RNNs and their structure; information flow through unrolled RNNs; forward and backpropagation in RNNs; issues in training (vanishing/exploding gradients) and mitigation strategies (gradient clipping, truncated BPTT, RMSprop); hidden states, weight sharing, and temporal dependencies; activation functions (tanh, ReLU); SimpleRNN implementation in Keras; time series forecasting with synthetic stock data; strengths and weaknesses of RNNs for temporal tasks |
| 7    | üéº Applications of RNNs and CNN-RNN Hybrids                   | Real-world RNN applications: time series forecasting (finance, weather), music generation, medical diagnostics, video frame prediction; combining CNNs with RNNs for spatial-temporal modeling; hybrid architectures (CNN‚ÜíRNN, RNN-CNN); case study on glaucoma detection using fundus image sequences; architecture design, dataset preparation, and model evaluation (F-measure); comparison between CNN-only and CNN+RNN models; discussion on temporal vs spatial feature extraction |
| 8    | üîÑ Advanced RNNs: LSTM Networks                                | Challenges in RNN training (vanishing/exploding gradients); BPTT and weight sharing; introduction to LSTM architecture: cell state, hidden state, forget/input/output gates; gate mechanics and equations; how LSTMs mitigate gradient issues; step-by-step cell computation and data flow; LSTM types: vanilla, stacked, bidirectional; use cases in long-sequence modeling; hands-on Keras implementation and training; performance comparison with basic RNNs |
| 9    | üîÑ Advanced RNNs: GRU Networks & Comparative Modeling          | GRU architecture: reset and update gates; detailed flow of GRU forward pass; benefits of GRU over LSTM in efficiency and performance; practical implementation of GRUs in Keras; sequence prediction case study (integer series); CNN-LSTM model hybrid for spatial-temporal modeling; case study: sine-cosine sequence modeling using RNN, LSTM, GRU; comparison by training loss, validation, gradient stability, and memory usage; conclusions on architecture trade-offs |
| 10   | ‚öñÔ∏è LSTM vs GRU: Weight Sharing, Efficiency & Training          | Weight-sharing principles in RNN, LSTM, and GRU (time step consistency, gate-specific weights); architectural comparison of LSTM (input, forget, output gates) vs GRU (update, reset gates); demo of weight-sharing and hidden state behavior; drawbacks of LSTMs (complexity, overfitting, hardware inefficiency); advantages of GRUs (fewer parameters, faster training, better regularization); real-world implications for training time, memory usage, and model stability |
| 11   | üëÅÔ∏è Attention Mechanisms and Transformer Foundations           | Limitations of RNNs and LSTMs in modeling long-range dependencies; introduction to attention mechanisms (selective focus, human cognition analogy); types of attention: self-attention, multi-head, additive (Bahdanau), and scaled dot-product (Luong); attention score functions (dot product, additive, multiplicative); context vectors and their computation; encoder-decoder architecture; role of weighted sums in capturing sequence context; applications in NLP, time series, and image captioning |
| 12   | ü§ñ Transformer Models and Vision Transformers (ViT)           | Transformer architecture and its components (self-attention, multi-head attention, feedforward layers, residual connections, layer normalization); encoder-decoder structure; positional encoding and its role in sequence modeling; comparison to RNNs and CNNs; vision-specific adaptation via Vision Transformers (ViT); implementation of Transformers in TensorFlow/Keras using MultiHeadAttention; CIFAR-10 classification with custom and pretrained ViT models; fine-tuning and transfer learning |
| 13   | üß™ Generative Adversarial Networks (GANs)                    | Introduction to generative models vs. discriminative models; GAN structure: generator and discriminator roles; adversarial training process and minimax game; challenges (mode collapse, vanishing gradients); key GAN variants: cGANs, DCGANs, StyleGAN, CycleGAN, InfoGAN; architecture and training algorithm walkthrough; applications in image generation, style transfer, data augmentation, and synthetic medical data; Alzheimer's case study using GANs with multi-modal brain imaging and multi-loop learning |
| 14   | üé§ Final Project Presentations                                       | Student presentations on final projects                                                                                                                                |

## üßæ Marking Scheme
| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

---

This course explores advanced deep learning concepts through hands-on implementation using TensorFlow and Keras. Students will gain experience with CNNs, RNNs (including LSTM and GRU), Transformer models, and GANs, while solving real-world problems and presenting their solutions.