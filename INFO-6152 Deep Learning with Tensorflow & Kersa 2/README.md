| Week | Title                                                                 | Key Topics Covered                                                                                                                                                      |
|------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1    | üß† Introduction to ANNs and Limitations in Image Processing          | Review of Artificial Neural Networks (ANNs): components (neurons, layers, weights/bias, activation functions, loss functions, optimizers); practical ANN implementation in Keras (model creation, compilation, training, and evaluation); digit recognition case study with MNIST dataset; performance metrics (accuracy, precision, recall); limitations of ANNs with high-dimensional image data (lack of spatial awareness, computational cost, overfitting, translation invariance); need for CNNs in image processing |
| 2    | üß± Fundamentals of CNNs and Hierarchical Feature Learning            | Introduction to CNNs and their advantages in image processing; structural design and grid data processing; core CNN components (convolutional, pooling, activation, fully connected, regularization); hierarchical feature learning (low-, mid-, high-level features); key CNN capabilities (translation invariance, parameter sharing, sparsity); comparison with ANNs for large image data; working pipeline of a CNN (preprocessing, layers, optimization); application domains (medical imaging, GIS, signal processing, video, seismic data); real-world models: LeNet, AlexNet, VGG, ResNet, Inception |
| 3    | ‚öôÔ∏è CNN Architecture, OpenCV & Data Augmentation                     | Convolutional layers and kernel operations; stride and padding types ('valid' vs. 'same'); pooling layers (max, average, global); forward and backpropagation processes; regularization techniques (dropout, batch normalization); OpenCV for image loading, preprocessing (resizing, normalization, color space conversion), and data augmentation; feature extraction (Canny, Sobel, Harris corners); visualizing images, filters, annotations; CNN implementation with TensorFlow on MNIST and CIFAR-10 datasets; TensorFlow + OpenCV integration pipeline; image augmentation using ImageDataGenerator |
| 4    | ‚öôÔ∏è CNN Applications, Transfer Learning & Object Detection           | Real-world applications of CNNs: image classification, object detection, segmentation; CNN model pipeline using TensorFlow and Keras; practical demo of image classification (fruit dataset); architecture components (input, convolutional, pooling, flatten, dense, output layers); training workflow (data normalization, augmentation, forward/backpropagation, loss, optimizer); evaluation metrics (accuracy, precision, recall, F1, confusion matrix); advanced CNN architectures: VGG16/19, ResNet, Inception, MobileNet; trade-offs in complexity, performance, and use cases; intro to transfer learning with pretrained models (ResNet50); object detection: R-CNN, Fast R-CNN, Faster R-CNN, YOLO; bounding box regression, IoU, confidence scoring, Non-Max Suppression (NMS); performance optimization |
| 5    | üîÅ Recurrent Neural Networks ‚Äì Concepts & Applications              | Limitations of CNNs for sequence modeling; introduction to RNNs and their structure; information flow through unrolled RNNs; forward and backpropagation in RNNs; issues in training (vanishing/exploding gradients); mitigation strategies (gradient clipping, truncated BPTT, RMSprop); hidden states, weight sharing, and temporal dependencies; activation functions (tanh, ReLU); SimpleRNN implementation in Keras; time series forecasting with synthetic stock data; real-world RNN applications: time series forecasting (finance, weather), music generation, medical diagnostics, video frame prediction; combining CNNs with RNNs (CNN‚ÜíRNN, RNN-CNN); case study on glaucoma detection using fundus image sequences; CNN-only vs. CNN+RNN performance (F-measure); spatial vs. temporal feature trade-offs |
| 6    | üîÑ LSTM Networks ‚Äì Deep Dive & Practical Modeling                   | Challenges in RNN training (vanishing/exploding gradients); BPTT and weight sharing; introduction to LSTM architecture: cell state, hidden state, forget/input/output gates; gate mechanics and equations; how LSTMs mitigate gradient issues; step-by-step cell computation and data flow; LSTM types: vanilla, stacked, bidirectional; use cases in long-sequence modeling; hands-on Keras implementation and training; performance comparison with basic RNNs |
| 7    | üîÑ GRU Networks & Comparative RNN Modeling                         | GRU architecture: reset and update gates; detailed flow of GRU forward pass; benefits of GRU over LSTM in efficiency and performance; practical implementation of GRUs in Keras; sequence prediction case study (integer series); CNN-LSTM hybrid modeling; sine-cosine sequence modeling with RNN, LSTM, and GRU; training loss, validation, memory usage; conclusions on architecture trade-offs |
| 8    | ‚öñÔ∏è LSTM vs GRU: Weight Sharing & Efficiency in RNNs                | Weight-sharing principles in RNN, LSTM, and GRU (time step consistency, gate-specific weights); architectural comparison of LSTM (input, forget, output gates) vs GRU (update, reset gates); demonstration of weight-sharing and hidden state behavior; limitations of LSTMs (complexity, overfitting, training difficulty); GRU advantages (fewer parameters, regularization, speed, hardware efficiency); real-world training implications |
| 9    | üëÅÔ∏è Attention Mechanisms and Transformer Foundations               | Limitations of RNNs and LSTMs in modeling long-range dependencies; introduction to attention mechanisms (selective focus, human cognition analogy); types of attention: self-attention, multi-head, additive (Bahdanau), and scaled dot-product (Luong); attention score functions (dot product, additive, multiplicative); context vectors and their computation; encoder-decoder architecture; role of weighted sums in capturing sequence context; applications in NLP, time series, and image captioning |
| 10   | ü§ñ Transformer Models and Vision Transformers (ViT)               | Transformer architecture and its components (self-attention, multi-head attention, feedforward layers, residual connections, layer normalization); encoder-decoder structure; positional encoding and its role in sequence modeling; comparison to RNNs and CNNs; vision-specific adaptation via Vision Transformers (ViT); implementation of Transformers in TensorFlow/Keras using MultiHeadAttention; CIFAR-10 classification with custom and pretrained ViT models; fine-tuning and transfer learning |
| 11   | üß™ Generative Adversarial Networks (GANs)                         | Introduction to generative models vs. discriminative models; GAN structure: generator and discriminator roles; adversarial training process and minimax game; challenges (mode collapse, vanishing gradients); key GAN variants: cGANs, DCGANs, StyleGAN, CycleGAN, InfoGAN; architecture and training algorithm walkthrough; applications in image generation, style transfer, data augmentation, and synthetic medical data; Alzheimer's case study using GANs with multi-modal brain imaging and multi-loop learning |
| 12   | üß™ GAN Implementation & Case Study (Alzheimer‚Äôs Diagnosis)       | GAN training algorithm walkthrough: discriminator and generator update cycles; dataset generation using random noise; detailed loss function tuning; backpropagation of adversarial loss; case study using BNLoop-GAN for Alzheimer's classification from MRI data; multi-loop learning algorithm; accuracy, sensitivity, specificity metrics; performance comparisons with single-modal vs. multi-modal networks; discussion of generalization in healthcare GAN models |
| 13   | üé§ Final Project Presentations                                    | Student presentations on final projects |
| 14   | üß™ Final Exam                                                     | theory and practical |

## üßæ Marking Scheme
| Component         | Weight |
|-------------------|--------|
| In-Class Activity (5 total) | 20%    |
| Quizzes (4 total) | 20%    |
| Final Project     | 30%    |
| Final Exam        | 30%    |

---

This course explores advanced deep learning concepts through hands-on implementation using TensorFlow and Keras. Students will gain experience with CNNs, RNNs (including LSTM and GRU), Transformer models, and GANs, while solving real-world problems and presenting their solutions.